import traceback
import faiss
import numpy as np
import os
import pandas as pd
from models.embeddings_model import EmbeddingModel
from sentence_transformers import SentenceTransformer, util

class FAISSVectorStore:
    def __init__(self, index_path="data/embeddings/faiss_index", reranker_model="sentence-transformers/msmarco-distilbert-base-v4"):
        self.index_path = index_path
        self.embedding_model = EmbeddingModel()
        self.reranker = SentenceTransformer(reranker_model)
        self.index = None

        # Carregar os chunks processados
        csv_path = "data/processed/results_extraction_chunks_updated.csv"
        if os.path.exists(csv_path):
            self.df_chunks = pd.read_csv(csv_path)
        else:
            raise FileNotFoundError(f"‚ùå Erro: Arquivo CSV {csv_path} n√£o encontrado!")

        # Criar um DataFrame com os intervalos de √≠ndices por concurso
        self.concurso_indices = self._calcular_indices_concursos()

        # Se o √≠ndice FAISS n√£o existir, cria um novo
        if not os.path.exists(self.index_path):
            print("‚ö†Ô∏è FAISS index n√£o encontrado! Criando novo...")
            self.create_index_from_chunks()

    def _calcular_indices_concursos(self):
        """Cria um dicion√°rio com os intervalos de √≠ndices de cada concurso."""
        concursos = self.df_chunks["Concurso"].dropna().unique()
        concurso_indices = {}

        print("\nconcursos: ", concursos)
        for concurso in concursos:
            indices = self.df_chunks[self.df_chunks["Concurso"] == concurso].index
            if len(indices) > 0:
                concurso_indices[concurso] = (indices.min(), indices.max())

        return concurso_indices

    def create_index(self, texts):
        """Cria um √≠ndice FAISS a partir de uma lista de textos."""
        print(f"üîπ Criando embeddings para {len(texts)} chunks...")
        embeddings = [self.embedding_model.get_embedding(text) for text in texts]
        embeddings = np.array(embeddings, dtype=np.float32)

        self.index = faiss.IndexFlatL2(embeddings.shape[1])
        self.index.add(embeddings)

        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
        faiss.write_index(self.index, self.index_path)
        print("‚úÖ FAISS index criado e salvo.")

    def create_index_from_chunks(self):
        """Cria um √≠ndice FAISS a partir do CSV `results_extraction_chunks_updated.csv`."""
        csv_path = "data/processed/results_extraction_chunks_updated.csv"
        if os.path.exists(csv_path):
            self.create_index(self.df_chunks["Chunk"].dropna().tolist())
        else:
            print("‚ùå Erro: Nenhum arquivo de chunks encontrado. Rode `extractor.py` primeiro.")

    def load_index(self):
        """Carrega o √≠ndice FAISS salvo."""
        if os.path.exists(self.index_path):
            self.index = faiss.read_index(self.index_path)
            print("‚úÖ FAISS index carregado.")
        else:
            print("‚ùå Erro: FAISS index n√£o encontrado! Execute `create_index_from_chunks()` primeiro.")

    def search(self, query, concurso=None, k=50, rerank_top_n=10):
        """Faz busca no FAISS, priorizando o concurso espec√≠fico e ampliando para global se necess√°rio."""
        try:
            query_embedding = np.array([self.embedding_model.get_embedding(query)], dtype=np.float32)
            distances, indices = self.index.search(query_embedding, k)

            print(f"üîç FAISS Retornou √çndices: {indices}")
            print(f"üîç FAISS Retornou Dist√¢ncias: {distances}")

            filtered_indices = indices[0].tolist()
            retrieved_texts = []

            # üîπ Filtro de concurso, se houver um identificado
            if concurso and concurso in self.concurso_indices:
                min_index, max_index = self.concurso_indices[concurso]
                filtered_indices = [i for i in filtered_indices if min_index <= i <= max_index]

            retrieved_texts = [self.df_chunks.iloc[i]["Chunk"] for i in filtered_indices if i < len(self.df_chunks)]

            # üîπ Se n√£o encontrou nada, buscar globalmente
            if not retrieved_texts:
                print(f"‚ö†Ô∏è Nenhum chunk encontrado para {concurso}. Tentando busca global...")
                filtered_indices = indices[0].tolist()
                retrieved_texts = [self.df_chunks.iloc[i]["Chunk"] for i in filtered_indices if i < len(self.df_chunks)]

            if not retrieved_texts:
                return ["‚ùå Nenhuma informa√ß√£o espec√≠fica encontrada para esse concurso."]

            # üîπ Melhorando o reranking com modelo mais avan√ßado
            rerank_scores = util.cos_sim(
                self.reranker.encode(query, convert_to_tensor=True),
                self.reranker.encode(retrieved_texts, convert_to_tensor=True)
            )
            ranked_results = sorted(zip(retrieved_texts, rerank_scores.tolist()), key=lambda x: x[1], reverse=True)

            best_chunks = [text for text, _ in ranked_results[:rerank_top_n]]
            return best_chunks

        except Exception as e:
            print(f"‚ùå Erro ao buscar no FAISS: {e}")
            traceback.print_exc()
            return []




    def print_faiss_status(self):
        if self.index is None:
            print("‚ùå FAISS index n√£o foi inicializado.")
        else:
            print(f"‚úÖ FAISS index cont√©m {self.index.ntotal} embeddings.")


# Teste
if __name__ == "__main__":
    store = FAISSVectorStore()
    store.load_index()
    query = "Quantas vagas est√£o abertas?"
    concurso = "FUNAI"
    results = store.search(query, concurso)
    print(f"üîç Melhores Chunks para {concurso}: {results}")
