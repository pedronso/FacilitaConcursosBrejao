import os
import pandas as pd
import time
from sklearn.metrics import accuracy_score, f1_score
from pipelines.rag import RAGPipeline

CSV_CHUNKS = "data/processed/results_extraction_chunks.csv"
METRICS_OUTPUT = "reports/metricas.csv"

def carregar_dados():
    """Carrega os chunks processados para teste."""
    if not os.path.exists(CSV_CHUNKS):
        print(f"‚ùå Arquivo {CSV_CHUNKS} n√£o encontrado! Certifique-se de que a etapa de extra√ß√£o foi conclu√≠da.")
        return None
    
    df = pd.read_csv(CSV_CHUNKS)
    
    if "Chunk" not in df.columns:
        print(f"‚ùå A coluna 'Chunks' n√£o est√° presente no arquivo {CSV_CHUNKS}.")
        return None
    
    return df

def avaliar_respostas(ground_truth, respostas_geradas):
    """Avalia as respostas geradas pelo RAG com base nas respostas esperadas."""
    if not ground_truth or not respostas_geradas:
        print("‚ö†Ô∏è Dados de teste insuficientes para avalia√ß√£o.")
        return {}

    # estamos comparando as respostas diretamente 
    # TODO: (pode ser ajustado para NLP, 
    # ou outro llm analisar se as respostas condizem ou n√£o, 
    # responder por exemplo unicamente com "y" ou "n, e fazermos esta contagem")
    accuracy = accuracy_score(ground_truth, respostas_geradas)
    f1 = f1_score(ground_truth, respostas_geradas, average="weighted")

    return {
        "Acur√°cia": accuracy,
        "F1-Score": f1
    }

def avaliar_sistema():
    """Executa a avalia√ß√£o do desempenho do chatbot."""
    print("\nüìä Iniciando avalia√ß√£o de m√©tricas...")

    start_time = time.time()
    
    df_chunks = carregar_dados()
    if df_chunks is None:
        return

    rag = RAGPipeline()

    perguntas_teste = [
        "Quais s√£o os concursos para engenheiros?",
        "Quais os prazos de inscri√ß√£o?",
        "Tem algum concurso para n√≠vel m√©dio?",
        "Quantas vagas est√£o abertas para o IBAMA?",
        "O MPU est√° com concursos abertos?"
    ]

    respostas_esperadas = [
        "Concursos para engenheiros incluem...",
        "Os prazos de inscri√ß√£o variam, mas...",
        "Atualmente, existem concursos de n√≠vel m√©dio como...",
        "O IBAMA tem 460 vagas abertas.",
        "Sim, o MPU anunciou vagas recentemente."
    ]

    respostas_geradas = []
    for pergunta in perguntas_teste:
        print(f"\nüîπ Pergunta: {pergunta}")
        resposta = rag.generate_answer(pergunta)
        print(f"üí¨ Resposta do modelo: {resposta}\n")
        respostas_geradas.append(resposta)

    # Avalia√ß√£o das respostas
    metricas = avaliar_respostas(respostas_esperadas, respostas_geradas)

    # Criar DataFrame de m√©tricas
    df_metricas = pd.DataFrame([metricas])
    
    # Salvar m√©tricas em CSV
    os.makedirs(os.path.dirname(METRICS_OUTPUT), exist_ok=True)
    df_metricas.to_csv(METRICS_OUTPUT, index=False)

    print(f"‚úÖ M√©tricas salvas em: {METRICS_OUTPUT}")

    total_time = time.time() - start_time
    print(f"\n‚è≥ Tempo total da avalia√ß√£o: {total_time:.2f} segundos.")
