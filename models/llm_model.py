from langchain_groq import ChatGroq
import os
import json
import requests
from dotenv import load_dotenv
import time

load_dotenv()

# Mapeamento de modelos com base no nome da pasta de configuraÃ§Ã£o.
CONFIG_MODEL_MAP = {
    "DeepSeek": "deepseek-r1-distill-llama-70b",
    "LLaMA": "llama3-8b-8192", #"llama3-70b-8192",
    "Mixtral": "mixtral-8x7b-32768",
}

# Lista de modelos de fallback para usar caso o modelo atual falhe.
FALLBACK_MODELS = [
    "lama-3.3-70b-versatile",
    "llama-3.1-8b-instant",
    "llama3-8b-8192",
    "mixtral-8x7b-32768", 
]

from langchain_groq import ChatGroq
import os
import json
import requests
from dotenv import load_dotenv
import time
from collections import deque

load_dotenv()

# Mapeamento de modelos com base no nome da pasta de configuraÃ§Ã£o.
CONFIG_MODEL_MAP = {
    "DeepSeek": "deepseek-r1-distill-llama-70b",
    "LLaMA": "llama3-8b-8192",
    "Mixtral": "mixtral-8x7b-32768",
}

# Lista de modelos de fallback para usar caso o modelo atual falhe.
FALLBACK_MODELS = [
    "lama-3.3-70b-versatile",
    "llama-3.1-8b-instant",
    "llama3-8b-8192",
    "mixtral-8x7b-32768", 
]

# Limites do modelo DeepSeek
REQUESTS_PER_MINUTE = 30
TOKENS_PER_MINUTE = 6000

# Controle de requisiÃ§Ãµes por minuto
request_timestamps = deque()

class LLMModel:
    def __init__(self, config_name):
        """
        Inicializa a conexÃ£o com o modelo LLM da Groq com base no nome da configuraÃ§Ã£o.
        Em caso de erro, tenta modelos de fallback.
        """
        self.config_name = config_name
        self.model_name = self.get_model_from_config(config_name)
        self.init_llm(self.model_name)

    def get_model_from_config(self, config_name):
        """
        ObtÃ©m o modelo correto com base no nome da pasta de configuraÃ§Ã£o.
        Exemplo: "DeepSeek_GTE-Large_200_40_ON_ON_ON" â†’ "deepseek-r1-distill-llama-70b"
        """
        for key, model in CONFIG_MODEL_MAP.items():
            if key.lower() in config_name.lower():
                return model
        return FALLBACK_MODELS[0]

    def init_llm(self, model_name):
        """Inicializa a API da Groq com o modelo especificado e armazena o modelo atual."""
        self.llm = ChatGroq(
            model=model_name,
            api_key=os.getenv("GROQ_API_KEY"),
            temperature=0,
            max_tokens=2000, 
            timeout=None,
            max_retries=2
        )
        self.current_model = model_name
        print(f"ğŸ”¹ Modelo carregado: {model_name}")

    def wait_for_rate_limit(self):
        """Garante que nÃ£o excedemos 30 requisiÃ§Ãµes por minuto."""
        while len(request_timestamps) >= REQUESTS_PER_MINUTE:
            elapsed_time = time.time() - request_timestamps[0]
            if elapsed_time < 60:
                sleep_time = 60 - elapsed_time
                print(f"â³ Esperando {sleep_time:.2f} segundos para respeitar o limite de requisiÃ§Ãµes por minuto...")
                time.sleep(sleep_time)
            request_timestamps.popleft()

    def generate_response(self, prompt):
        """
        Gera uma resposta utilizando o modelo LLM.
        - **Se tokens do input excederem 6000**, reduz o input pela metade.
        - **Se atingir o rate limit de 30 requisiÃ§Ãµes por minuto**, aguarda.
        - **Se ocorrer erro, tenta um fallback automÃ¡tico**.
        """
        wait_time = 30
        original_prompt = prompt  # Guarda o prompt original para ajustes futuros

        while True:
            self.wait_for_rate_limit()  # Garante que nÃ£o excedemos o limite de requisiÃ§Ãµes
            
            try:
                estimated_output_tokens = 1500  # SuposiÃ§Ã£o inicial do tamanho da resposta
                total_tokens = len(prompt.split()) + estimated_output_tokens
                
                # Se ultrapassarmos os 6000 tokens permitidos, reduzimos o input pela metade
                while total_tokens > TOKENS_PER_MINUTE:
                    print("âš ï¸ Excesso de tokens! Reduzindo input pela metade...")
                    prompt = " ".join(prompt.split()[:len(prompt.split()) // 2])
                    total_tokens = len(prompt.split()) + estimated_output_tokens
                
                request_timestamps.append(time.time())  # Registra o timestamp da requisiÃ§Ã£o
                
                response = self.llm.invoke([("system", "Responda com precisÃ£o."), ("human", prompt)]).content
                return f"[Model used: {self.current_model}] {response}"

            except Exception as e:
                error_msg = str(e)
                print(f"âŒ Erro ao gerar resposta: {error_msg}")

                if "rate_limit_exceeded" in error_msg:
                    print(f"â³ Esperando {wait_time} segundos devido a rate limit...")
                    time.sleep(wait_time)
                    wait_time += 30

                elif "max_tokens" in error_msg or "token limit exceeded" in error_msg or "context length exceeded" in error_msg:
                    print("âš ï¸ Excesso de tokens: reduzindo prompt pela metade e tentando novamente...")
                    prompt = " ".join(original_prompt.split()[:len(original_prompt.split()) // 2])

                else:
                    print("ğŸ”„ Tentando fallback para outro modelo...")
                    if FALLBACK_MODELS:
                        new_model = FALLBACK_MODELS.pop(0)
                        print(f"âš¡ Mudando para modelo alternativo: {new_model}")
                        self.init_llm(new_model)
                    else:
                        return f"Erro ao gerar resposta: {error_msg}"
    
    def generate_response_old(self, prompt):
            context = "IBGE - ANEXO I â€“ Quadro de Vagas e Postos de InscriÃ§Ã£o {â€˜FunÃ§Ã£oâ€™: â€˜APMâ€™, â€˜UFâ€™: â€˜ESâ€™, â€˜MunicÃ­pioâ€™: â€˜Cariacicaâ€™, â€˜VAGASâ€™: â€˜2â€™, â€˜ACâ€™: â€˜1â€™, â€˜PPPâ€™: â€˜1â€™, â€˜PcDâ€™: â€˜0â€™, â€˜EndereÃ§o para inscriÃ§Ãµesâ€™: â€˜Av. Nossa Sra. dos Navegantes, 675 (EdifÃ­cio PalÃ¡cio do CafÃ©), 9Âº andar - Enseada do SuÃ¡. VitÃ³ria/ESâ€™}, {â€˜FunÃ§Ã£oâ€™: â€˜APMâ€™, â€˜UFâ€™: â€˜ESâ€™, â€˜MunicÃ­pioâ€™: â€˜VitÃ³riaâ€™, â€˜VAGASâ€™: â€˜12â€™, â€˜ACâ€™: â€˜8â€™, â€˜PPPâ€™: â€˜2â€™, â€˜PcDâ€™: â€˜2â€™, â€˜EndereÃ§o para inscriÃ§Ãµesâ€™: â€˜Av. Nossa Sra. dos Navegantes, 675 (EdifÃ­cio PalÃ¡cio do CafÃ©), 9Âº andar - Enseada do SuÃ¡. VitÃ³ria/ESâ€™}, {â€˜FunÃ§Ã£oâ€™: â€˜APMâ€™, â€˜UFâ€™: â€˜MGâ€™, â€˜MunicÃ­pioâ€™: â€˜Ituramaâ€™, â€˜VAGASâ€™: â€˜2â€™, â€˜ACâ€™: â€˜1â€™, â€˜PPPâ€™: â€˜1â€™, â€˜PcDâ€™: â€˜0â€™, â€˜EndereÃ§o para inscriÃ§Ãµesâ€™: â€˜Rua Armando Fratari, 867, Vila OlÃ­mpica. Iturama/MGâ€™}, {â€˜FunÃ§Ã£oâ€™: â€˜APMâ€™, â€˜UFâ€™: â€˜MSâ€™, â€˜MunicÃ­pioâ€™: â€˜Campo Grandeâ€™, â€˜VAGASâ€™: â€˜7â€™, â€˜ACâ€™: â€˜6â€™, â€˜PPPâ€™: â€˜1â€™, â€˜PcDâ€™: â€˜0â€™, â€˜EndereÃ§o para inscriÃ§Ãµesâ€™: â€˜Rua BarÃ£o do Rio Branco, 1431, Centro. Campo Grande/MSâ€™}, {â€˜FunÃ§Ã£oâ€™: â€˜APMâ€™, â€˜UFâ€™: â€˜SCâ€™, â€˜MunicÃ­pioâ€™: â€˜ConcÃ³rdiaâ€™, â€˜VAGASâ€™: â€˜5â€™, â€˜ACâ€™: â€˜4â€™, â€˜PPPâ€™: â€˜1â€™, â€˜PcDâ€™: â€˜0â€™, â€˜EndereÃ§o para inscriÃ§Ãµesâ€™: â€˜Rua Marechal Deodoro, 772, Centro. ConcÃ³rdia/SCâ€™}, {â€˜FunÃ§Ã£oâ€™: â€˜APMâ€™, â€˜UFâ€™: â€˜SCâ€™, â€˜MunicÃ­pioâ€™: â€˜Rio do Sulâ€™, â€˜VAGASâ€™: â€˜1â€™, â€˜ACâ€™: â€˜1â€™, â€˜PPPâ€™: â€˜0â€™, â€˜PcDâ€™: â€˜0â€™, â€˜EndereÃ§o para inscriÃ§Ãµesâ€™: â€˜Rua Tuiuti, 20, salas 401 e 402, Centro. Rio do Sul/SCâ€™}, {â€˜FunÃ§Ã£oâ€™: â€˜SCQâ€™, â€˜UFâ€™: â€˜SCâ€™, â€˜MunicÃ­pioâ€™: â€˜Palmitosâ€™, â€˜VAGASâ€™: â€˜1â€™, â€˜ACâ€™: â€˜1â€™, â€˜PPPâ€™: â€˜0â€™, â€˜PcDâ€™: â€˜0â€™, â€˜EndereÃ§o para inscriÃ§Ãµesâ€™: â€˜Rua Visconde de Rio Branco, 932, sala 102, Centro. Palmitos/SCâ€™}"
            prompt1 = f"Baseando-se somente nos seguintes textos:{context}\n\n responda: Quais sÃ£o os quadros de vagas e postos de inscriÃ§Ã£o para o ibge?"
            system = """VocÃª Ã© um assistente especializado em concursos pÃºblicos e estudos para provas.  
    Responda **apenas em portuguÃªs** e de forma **clara e objetiva**.  

    ğŸ“Œ **Regras de resposta:**  
    - Se a pergunta nÃ£o estiver relacionada a concursos ou estudos, diga que nÃ£o pode ajudar.  
    - Se nÃ£o souber a resposta, apenas diga **"NÃ£o tenho essa informaÃ§Ã£o."**  
    - Se houver repetiÃ§Ã£o de informaÃ§Ãµes no contexto, mencione cada item **apenas uma vez**.  
    - Se houver leis, artigos ou regras nos editais, priorize a informaÃ§Ã£o mais recente.  
    - Sempre que possÃ­vel, explique **com base no edital e em regras oficiais**.  

    ğŸ” **Exemplo de comportamento esperado:**  
    âœ… **UsuÃ¡rio:** "Quantas vagas hÃ¡ para o cargo X?"  
    âœ… **Assistente:** "O edital informa que hÃ¡ 30 vagas para o cargo X."  

    ğŸš« **UsuÃ¡rio:** "Me fale sobre esportes."  
    ğŸš« **Assistente:** "Eu sou especializado apenas em concursos pÃºblicos e estudos."  
    """
            messages = [('system', system),
                        ("human", prompt)]
            try:
                resposta = self.llm.invoke(messages).content
            except Exception as e:
                print(f"Erro ao processar: {e}")

            return resposta

class LocalLLMModel:
    def __init__(self, model_name="llama3.2:latest", url="http://localhost:11434/api/generate"):
        self.model_name = model_name
        self.url = url

    def generate_response(self, prompt):
        payload = {
            'model': self.model_name,
            'system': "VocÃª Ã© um especialista em concursos pÃºblicos, ajude a responder as dÃºvidas, e caso nÃ£o saiba, nÃ£o responda.",
            'prompt': prompt,
        }
        llm_response = ''
        with requests.post(self.url, json=payload, stream=True) as response:
            for line in response.iter_lines():
                if line:
                    data = json.loads(line.decode("utf-8"))
                    llm_response += data.get("response", "")
        return llm_response

class LLMReviewerModel(LLMModel):
    def __init__(self, config_name="llama3-8b-8192"):
        super().__init__(config_name)

    def generate_response(self, prompt):
        system = """VocÃª Ã© um avaliador de respostas. Sua tarefa Ã© analisar a qualidade de uma resposta com base na pergunta feita e atribuir uma nota de 0 a 10, considerando os seguintes critÃ©rios:

1. **PrecisÃ£o**: A resposta estÃ¡ correta e alinhada com a pergunta?
2. **Completude**: A resposta fornece todas as informaÃ§Ãµes necessÃ¡rias?
3. **RelevÃ¢ncia**: A resposta Ã© relevante para a pergunta feita?
4. **Clareza**: A resposta Ã© clara e bem estruturada?

### **InstruÃ§Ãµes**
- Atribua uma nota de **0 a 10** para a resposta, exclusivamente.
- Responda **APENAS COM O NÃšMERO DA NOTA**, sem justificativas, explicaÃ§Ãµes ou texto adicional.
- Penalize respostas vagas, evasivas ou que indicam falta de informaÃ§Ã£o:
  - Se a resposta disser "NÃ£o sei" ou nÃ£o fornecer qualquer dado Ãºtil, a nota deve ser **0**.
  - Se a resposta for genÃ©rica e nÃ£o agregar valor, a nota deve ser no mÃ¡ximo **2**.
  - Respostas parcialmente corretas, mas com falta de detalhes essenciais, devem ter notas entre **3 e 6**.
  - Apenas respostas completas e corretas devem receber **8 a 10**.

### **Exemplos**
Pergunta: "Qual Ã© a data da inscriÃ§Ã£o para o concurso da AeronÃ¡utica?"
Resposta: "O perÃ­odo de inscriÃ§Ã£o para o concurso da AeronÃ¡utica (CFS 1/2026) Ã© de 15/01/2025 a 14/02/2025."
Nota: 10

Pergunta: "Qual Ã© o salÃ¡rio para o concurso da AeronÃ¡utica?"
Resposta: "O texto fornecido nÃ£o especifica o salÃ¡rio para o concurso da AeronÃ¡utica. Para obter informaÃ§Ãµes sobre o salÃ¡rio, Ã© recomendado consultar o site oficial da AeronÃ¡utica ou entrar em contato com o ServiÃ§o de Recrutamento e Preparo de Pessoal da AeronÃ¡utica (SEREP) pelos telefones fornecidos no texto."
Nota: 2

Pergunta: "Qual Ã© a data da inscriÃ§Ã£o para o concurso da AeronÃ¡utica?"
Resposta: "NÃ£o sei."
Nota: 0

Pergunta: "Quais sÃ£o os requisitos para o concurso?"
Resposta: "Os requisitos variam conforme o edital. Recomenda-se verificar diretamente no site da AeronÃ¡utica."
Nota: 1
"""
        messages = [('system', system), ("human", prompt)]
        try:
            response = self.llm.invoke(messages).content
            return response
        except Exception as e:
            print(f"Erro ao processar: {e}")
            return f"Erro: {e}"

# Teste
if __name__ == "__main__":
    model = LLMModel("DeepSeek_GTE-Large_200_40_ON_ON_ON")
    resposta = model.generate_response("Quais sÃ£o os concursos pÃºblicos com inscriÃ§Ãµes abertas?")
    print(f"Resposta do modelo: {resposta}")
