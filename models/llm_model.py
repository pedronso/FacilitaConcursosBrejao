from langchain_groq import ChatGroq
import os
from dotenv import load_dotenv

load_dotenv()

class LLMModel:
    def __init__(self, model_name="mixtral-8x7b-32768"):
        """
        Inicializa a conex√£o com o modelo LLM da Groq.
        - model_name: Modelo a ser utilizado (ex: `llama3-70b-8192` ou `mixtral-8x7b-32768`).
        """
        self.llm = ChatGroq(
            model=model_name,
            api_key=os.getenv("GROQ_API_KEY"),
            temperature=0,
            max_tokens=2500,  # üîπ Agora limitamos para 2500 tokens por requisi√ß√£o
            timeout=None,
            max_retries=2
        )

    def generate_response(self, prompt):
        """Gera uma resposta baseada no prompt fornecido."""
        messages = [("system", "Voc√™ √© um assistente especializado em concursos p√∫blicos."),
                    ("human", prompt)]
        return self.llm.invoke(messages).content  # Retorna apenas o conte√∫do da resposta

# Teste
if __name__ == "__main__":
    model = LLMModel()
    resposta = model.generate_response("Quais s√£o os concursos p√∫blicos com inscri√ß√µes abertas?")
    print(f"Resposta do modelo: {resposta}")
